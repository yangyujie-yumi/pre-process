{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Handling Numerical Data\n",
    "\n",
    "### Introduction\n",
    "cover numerous strategies for transforming raw numerical data into features purpose-built for machine learning algorithms\n",
    "### 讨论将原始数值数据转换为专门为机器学习算法构建的特征的策略\n",
    "### 4.1 Rescaling a feature\n",
    "Use scikit-learn's `MinMaxScaler` to rescale a feature array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28571429],\n",
       "       [0.35714286],\n",
       "       [0.42857143],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# create a feature\n",
    "feature = np.array([\n",
    "    [-500.5],\n",
    "    [-100.1],\n",
    "    [0],\n",
    "    [100.1],\n",
    "    [900.9]\n",
    "])\n",
    "\n",
    "# create scaler\n",
    "minmax_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# scale feature\n",
    "scaled_feature = minmax_scaler.fit_transform(feature)\n",
    "\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "重缩放是机器学习中常见的预处理任务。本书后面描述的许多算法都假设所有特性都在同一个尺度上，通常是0到1或-1到1。有许多重缩放技术，但最简单的一种称为最小-最大缩放。最小最大缩放使用特征的最小值和最大值将值重新缩放到某个范围内\n",
    "$$\n",
    "x_i^` = \\frac{x_i - min(x)}{max(x) - min(x)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### See Also\n",
    "* Feature scaling, wikipedia (https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "\n",
    "### 4.2 Standardizing a Feature\n",
    "scikit-learn's `StandardScaler` transforms a feature to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76058269],\n",
       "       [-0.54177196],\n",
       "       [-0.35009716],\n",
       "       [-0.32271504],\n",
       "       [ 1.97516685]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# create a feature\n",
    "feature = np.array([\n",
    "    [-1000.1],\n",
    "    [-200.2],\n",
    "    [500.5],\n",
    "    [600.6],\n",
    "    [9000.9]\n",
    "])\n",
    "\n",
    "# create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# transform the feature\n",
    "standardized = scaler.fit_transform(feature)\n",
    "\n",
    "standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "最小-最大缩放的一个常见替代方法是将特征重新缩放为近似标准的正态分布。为了实现这一点，使用标准化来转换数据，使其具有平均值x′或0和标准偏差σ1。具体地说，特征中的每个元素都经过如下转换\n",
    "$$\n",
    "x_i^` = \\frac{x_i - \\bar x}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化是机器学习预处理的一种常用的缩放方法，它比minmax scaling使用得多。但这取决于学习算法。例如，主成分分析通常使用标准化更好地工作，而最小-最大标度通常推荐用于神经网络。一般来说，默认标准化，它也叫做z-score。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 0.0\n",
      "Standard Deviation: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean {}\".format(round(standardized.mean())))\n",
    "print(\"Standard Deviation: {}\".format(standardized.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们的数据有显著的异常值，它会通过影响特征的平均值和方差对我们的标准化产生负面影响。在这种情况下，使用中间值和四分位数范围来重新缩放特征通常很有帮助。在scikit learn中，我们使用RobustScaler方法来执行此操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87387612],\n",
       "       [-0.875     ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [10.61488511]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create scaler\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# transform feature\n",
    "robust_scaler.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Normalizing Observations\n",
    "Use scikit-learn's `Normalizer` to rescale the feature values to have unit norm (a total length of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# create feature matrix\n",
    "features = np.array([\n",
    "    [0.5, 0.5],\n",
    "    [1.1, 3.4]\n",
    "])\n",
    "\n",
    "# create normalizer\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "\n",
    "# transofmr feature matrix\n",
    "normalizer.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "Many rescaling methods operate of features; however, we can also rescale across individual observations. `Normalizer` rescales the values on individual observations to have unit norm (the sum of their lengths is 1). This type of rescaling is often used when we have many equivalent features (e.g. text-classification when every word is n-word group is a feature).\n",
    "\n",
    "`Normalizer` provides three norm options with Euclidean norm (often called L2) being the default:\n",
    "$$\n",
    "||x||_2 = \\sqrt{x_1^2 + x_2^2 + ... + x_n^2}\n",
    "$$\n",
    "\n",
    "where x is an individual observation and x_n is that observation's value for the nth feature.\n",
    "\n",
    "Alternatively, we can specify Manhattan norm (L1):\n",
    "$$\n",
    "||x||_1 = \\sum_{i=1}^n{x_i}\n",
    "$$\n",
    "\n",
    "Intuitively, L2 norm can be thought of as the distance between two poitns in New York for a bird (i.e. a straight line), while L1 can be thought of as the distance for a human wlaking on the street (walk north one block, east one block, north one block, east one block, etc), which is why it is called \"Manhattan norm\" or \"Taxicab norm\".\n",
    "\n",
    "Practically, notice that `norm='l1'` rescales an observation's values so they sum to 1, which can sometimes be a desirable quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the first observation's values: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.24444444, 0.75555556]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform feature matrix\n",
    "features_l1_norm = Normalizer(norm=\"l1\").transform(features)\n",
    "print(\"Sum of the first observation's values: {}\".format(features_l1_norm[0,0] + features_l1_norm[0,1]))\n",
    "features_l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Generating Polynomial and Interaction Features\n",
    "\n",
    "You want to create polynominal and interaction features.\n",
    "Solution\n",
    "\n",
    "Even though some choose to create polynomial and interaction features manually, scikit-learn offers a built-in method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  4.,  4.,  8., 16.],\n",
       "       [ 2.,  3.,  4.,  6.,  9.],\n",
       "       [ 2.,  8.,  4., 16., 64.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create feature matrix\n",
    "features = np.array([[2, 4],\n",
    "                     [2, 3],\n",
    "                     [2, 8]])\n",
    "\n",
    "# Create PolynomialFeatures object\n",
    "polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Create polynomial features\n",
    "polynomial_interaction.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree parameter determines the maximum degree of the polynomial. For\n",
    "example, degree=2 will create new features raised to the second power:\n",
    "\n",
    "$$\n",
    "x_1,x_2,{x_1}^2,{x_2}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restrict the features created to only interaction features by setting interaction_only to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  4.,  8.],\n",
       "       [ 2.,  3.,  6.],\n",
       "       [ 2.,  8., 16.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction = PolynomialFeatures(degree=2,\n",
    "            interaction_only=True, include_bias=False)\n",
    "interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Polynomial features are often created when we want to include the notion that there exists a nonlinear relationship between the features and the target. For example, we might suspect that the effect of age on the probability of having a major medical con‐ dition is not constant over time but increases as age increases. We can encode that nonconstant effect in a feature, $x$, by generating that feature’s higher-order forms ($x^2$, $x^3$, etc.).\n",
    "\n",
    "Additionally, often we run into situations where the effect of one feature is dependent on another feature. \n",
    "\n",
    "A simple example would be if we were trying to predict whether or not our coffee was sweet and we had two features:\n",
    "\n",
    "1) whether or not the coffee was stirred and\n",
    "\n",
    "2) if we added sugar. Individually, each feature does not predict coffee sweetness, but the combination of their effects does. \n",
    "\n",
    "That is, a coffee would only be sweet if the coffee had sugar and was stirred. The effects of each feature on the target (sweetness) are dependent on each other. We can encode that relationship by including an interaction feature that is the product of the individual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Transforming Features\n",
    "\n",
    "You want to make a custom transformation to one or more features.\n",
    "\n",
    "In scikit-learn, use FunctionTransformer to apply a function to a set of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 13],\n",
       "       [12, 13],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Create feature matrix\n",
    "features = np.array([[2, 3],\n",
    "                     [2, 3],\n",
    "                     [2, 3]])\n",
    "    \n",
    "# Define a simple function\n",
    "def add_ten(x): return x + 10\n",
    "    \n",
    "# Create transformer\n",
    "ten_transformer = FunctionTransformer(add_ten) \n",
    "\n",
    "# Transform feature matrix\n",
    "ten_transformer.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the same transformation in pandas using apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0         12         13\n",
       "1         12         13\n",
       "2         12         13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
    "    \n",
    "# Apply function\n",
    "df.apply(add_ten)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "It is common to want to make some custom transformations to one or more features. For example, we might want to create a feature that is the natural log of the values of the different feature. We can do this by creating a function and then mapping it to features using either scikit-learn’s FunctionTransformer or pandas’ apply. In the sol‐ ution we created a very simple function, add_ten, which added 10 to each input, but there is no reason we could not define a much more complex function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Detecting Outliers\n",
    "\n",
    "You want to identify extreme observations.\n",
    "\n",
    "Detecting outliers is unfortunately more of an art than a science. However, a common method is to assume the data is normally distributed and based on that assumption “draw” an ellipse around the data, classifying any observation inside the ellipse as an inlier (labeled as 1) and any observation outside the ellipse as an outlier (labeled as -1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_blobs函数是为聚类产生数据集，产生一个数据集和相应的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e+04, -2.76017908e+00, -1.61734616e+00, -5.25790464e-01,\n",
       "        8.52518583e-02, -7.94152277e-01, -1.34052081e+00, -1.98197711e+00,\n",
       "       -2.18773166e+00, -1.97451969e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope \n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create simulated data\n",
    "features, _ = make_blobs(n_samples = 10,\n",
    "                             n_features = 2,\n",
    "                             centers = 1,\n",
    "                             random_state = 1)\n",
    "\n",
    "\n",
    "# Replace the first observation's values with extreme values\n",
    "features[0,0] = 10000\n",
    "features[0,1] = 10000\n",
    "features\n",
    "# Create detector\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "\n",
    "# Fit detector\n",
    "outlier_detector.fit(features) \n",
    "\n",
    "# Predict outliers\n",
    "outlier_detector.predict(features)\n",
    "type(features)\n",
    "feature = features[:,0]\n",
    "feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法的一个主要局限性是需要指定一个污染参数，即异常值的观察值的比例-一个我们不知道的值。把污染看作是我们对数据清洁度的估计。如果我们期望我们的数据有很少的异常值，我们可以将污染设置为较小的值。但是，如果我们认为数据很可能有异常值，我们可以将其设置为更高的值。              我们可以用四分位值而不是整体来观察这些特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one feature\n",
    "feature = features[:,0]\n",
    "\n",
    "# Create a function to return index of outliers\n",
    "def indicies_of_outliers(x):\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr=q3-q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    return np.where((x > upper_bound) | (x < lower_bound))\n",
    "\n",
    "# Run function\n",
    "indicies_of_outliers(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR是一组数据的第一个和第三个四分位数之间的差。异常值通常定义为小于第一个四分位数的1.5个IQR值或大于第三个四分位数的1.5个IQR值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "There is no single best technique for detecting outliers. Instead, we have a collection of techniques all with their own advantages and disadvantages. Our best strategy is often trying multiple techniques (e.g., both 'EllipticEnvelope' and IQR-based detection) and looking at the results as a whole.\n",
    "\n",
    "If at all possible, we should take a look at observations we detect as outliers and try to understand them. For example, if we have a dataset of houses and one feature is number of rooms, is an outlier with 100 rooms really a house or is it actually a hotel that has been misclassified? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Handling Outliers\n",
    "\n",
    "Typically we have three strategies we can use to handle outliers. \n",
    "\n",
    "First, we can drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Bathrooms  Square_Feet\n",
       "0   534433        2.0         1500\n",
       "1   392333        3.5         2500\n",
       "2   293222        2.0         1500\n",
       "3  4322032      116.0        48000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "houses = pd.DataFrame()\n",
    "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
    "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
    "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
    "\n",
    "# Filter observations\n",
    "houses[houses['Bathrooms'] < 20]\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Second, we can mark them as outliers and include it as a feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "      <th>Outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Bathrooms  Square_Feet  Outlier\n",
       "0   534433        2.0         1500        0\n",
       "1   392333        3.5         2500        0\n",
       "2   293222        2.0         1500        0\n",
       "3  4322032      116.0        48000        1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import numpy as np\n",
    "\n",
    "# Create feature based on boolean condition\n",
    "houses[\"Outlier\"] = np.where(houses[\"Bathrooms\"] < 20, 0, 1)\n",
    "\n",
    "# Show data\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we can transform the feature to dampen the effect of the outlier:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "      <th>Outlier</th>\n",
       "      <th>Log_Of_Square_Feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.778956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Bathrooms  Square_Feet  Outlier  Log_Of_Square_Feet\n",
       "0   534433        2.0         1500        0            7.313220\n",
       "1   392333        3.5         2500        0            7.824046\n",
       "2   293222        2.0         1500        0            7.313220\n",
       "3  4322032      116.0        48000        1           10.778956"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log feature\n",
    "houses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\"]] \n",
    "\n",
    "# Show data\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "与检测异常值类似，处理异常值没有硬性规则。从两个方面来处理。首先，应该考虑是什么使它们成为异常值。如果认为它们是数据中的错误，例如来自损坏的传感器或错误编码的值，那么可能会放弃观察值或用NaN替换异常值，因为我们无法相信这些值。但是，如果我们认为异常值是真正的极值（例如，有200个浴室的房子[豪宅]），则将其标记为异常值或转换其值更为合适。             \n",
    "第二，我们如何处理异常值应该基于我们的机器学习目标。例如，如果我们想根据房子的特点来预测房价，我们可以合理地假设拥有100多个浴室的豪宅的价格是由不同于普通家庭住宅的动态驱动的。此外，如果我们正在培训一个模型来作为在线住房贷款网络应用程序的一部分，我们可能会假设，我们的潜在用户不会包括那些想要购买豪宅的亿万富翁。              \n",
    "另外一点：如果有异常值，标准化可能不合适，因为平均值和方差可能会受到异常值的高度影响。在这种情况下，使用一种对异常值更健壮的重缩放方法，比如RobustScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Discretizating Features\n",
    "\n",
    "You have a numerical feature and want to break it up into discrete bins.\n",
    "\n",
    "Depending on how we want to break up the data, there are two techniques we can use. \n",
    "\n",
    "First, we can binarize the feature according to some threshold:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Create feature\n",
    "age = np.array([[6],\n",
    "                [12],\n",
    "                [20],\n",
    "                [36],\n",
    "                [65]])\n",
    "\n",
    "# Create binarizer\n",
    "binarizer = Binarizer(18)\n",
    "\n",
    "# Transform feature\n",
    "binarizer.fit_transform(age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Second, we can break up numerical features according to multiple thresholds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bin feature\n",
    "np.digitize(age, bins=[20,30,64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，bins参数的参数表示每个bin的左边缘。             \n",
    "例如，20参数不包含值为20的元素，只包含小于20的两个值。我们可以通过将参数设置为True来切换这种行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bin feature\n",
    "np.digitize(age, bins=[20,30,64], right=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Discretization can be a fruitful strategy when we have reason to believe that a numeri‐ cal feature should behave more like a categorical feature. For example, we might believe there is very little difference in the spending habits of 19- and 20-year-olds, but a significant difference between 20- and 21-year-olds (the age in the United States when young adults can consume alcohol). In that example, it could be useful to break up individuals in our data into those who can drink alcohol and those who cannot. Similarly, in other cases it might be useful to discretize our data into three or more bins.\n",
    "\n",
    "In the solution, we saw two methods of discretization—scikit-learn’s Binarizer for two bins and NumPy’s digitize for three or more bins—however, we can also use digitize to binarize features like Binarizer by only specifying a single threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bin feature\n",
    "np.digitize(age, bins=[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Grouping Observations Using Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to cluster observations so that similar observations are grouped together.\n",
    "\n",
    "If you know that you have k groups, you can use k-means clustering to group similar observations and output a new feature containing each observation’s group membership:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.877554</td>\n",
       "      <td>-3.336145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.287210</td>\n",
       "      <td>-8.353986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.943061</td>\n",
       "      <td>-7.023744</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.440167</td>\n",
       "      <td>-8.791959</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.641388</td>\n",
       "      <td>-8.075888</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  group\n",
       "0  -9.877554  -3.336145      0\n",
       "1  -7.287210  -8.353986      2\n",
       "2  -6.943061  -7.023744      2\n",
       "3  -7.440167  -8.791959      2\n",
       "4  -6.641388  -8.075888      2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "#产生50个数据点，两个属性特征，三个聚类\n",
    "features, _ = make_blobs(n_samples = 50,\n",
    "                         n_features = 2,\n",
    "                         centers = 3,\n",
    "                         random_state = 1)\n",
    "\n",
    "df = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
    "\n",
    "# make k-means clusterer\n",
    "clusterer = KMeans(3, random_state=0)\n",
    "\n",
    "# fit clusterer\n",
    "clusterer.fit(features)\n",
    "\n",
    "# predict values\n",
    "df['group'] = clusterer.predict(features)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "We are jumping ahead of ourselves a bit and will go much more in depth about clustering algorithms later in the book. However, I wanted to point out that we can use clustering as a preprocessing step. \n",
    "\n",
    "Specifically, we use unsupervised learning algorithms like k-means to cluster observations into groups. The end result is a categorical feature with similar observations being members of the same group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.10 Deleteing Observations with Missing Values\n",
    "\n",
    "You need to delete observations containing missing values.\n",
    "\n",
    "Deleting observations with missing values is easy with a clever line of NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1, 11.1],\n",
       "       [ 2.2, 22.2],\n",
       "       [ 3.3, 33.3]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = np.array([\n",
    "    [1.1, 11.1],\n",
    "    [2.2, 22.2],\n",
    "    [3.3, 33.3],\n",
    "    [np.nan, 55]\n",
    "])\n",
    "\n",
    "# keep only observations that are not (denoted by ~) missing\n",
    "features[~np.isnan(features).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can drop missing observations using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.2</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0        1.1       11.1\n",
       "1        2.2       22.2\n",
       "2        3.3       33.3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "Most machine learnign algorithms cannot handling any missing values in the target and feature arrays. The simplest solution is the delete every observation that contains one or more missing values\n",
    "\n",
    "There are three types of missing data:\n",
    "\n",
    "*Missing Completely At Random (MCAR)*\n",
    "* The probability that a value is missing is independent of everything.\n",
    "\n",
    "*Missing At Random (MAR)*\n",
    "* The probability that a value is missing is not completely random, but depends on information capture in other feature\n",
    "\n",
    "*Missing Not At Random (MNAR)*\n",
    "* The probability that a value is missing is not random and depends on information not captured in our features\n",
    "\n",
    "#### See Also\n",
    "* Identifying the Three Types of Missing Data (https://measuringu.com/missing-data/)\n",
    "* Missing-Data Imputation (http://www.stat.columbia.edu/~gelman/arm/missing.pdf)\n",
    "\n",
    "### 4.11 Imputing Missing Values\n",
    "### 填补缺失值\n",
    "You have missing values in your data and want to fill in or predict their values.\n",
    "\n",
    "If you have a small amount of data, predict the missing values using k-nearest neighbors (KNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.8730186113995938\n",
      "Imputed Value: -3.058372724614996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# make fake data\n",
    "features, _ = make_blobs(n_samples = 1000,\n",
    "                        n_features = 2,\n",
    "                        random_state = 1)\n",
    "\n",
    "# standardize the features\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "standardized_features\n",
    "\n",
    "# replace the first feature's first value with a missing value\n",
    "true_value = standardized_features[0, 0]\n",
    "true_value\n",
    "standardized_features[0,0] = np.nan\n",
    "\n",
    "# create imputer\n",
    "mean_imputer = Imputer(strategy=\"mean\", axis=0)\n",
    "\n",
    "# impute values\n",
    "feautres_mean_imputed = mean_imputer.fit_transform(features)\n",
    "\n",
    "# compare true and imputed values\n",
    "print(\"True Value: {}\".format(true_value))\n",
    "print(\"Imputed Value: {}\".format(feautres_mean_imputed[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use scikit-learn’s Imputer module to fill in missing values with the feature’s mean, median, or most frequent value. However, we will typically get worse results than KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.8730186113995938\n",
      "Imputed Value: -3.058372724614996\n"
     ]
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create imputer\n",
    "mean_imputer = Imputer(strategy=\"mean\", axis=0) \n",
    "\n",
    "# Impute values\n",
    "features_mean_imputed = mean_imputer.fit_transform(features)\n",
    "\n",
    "# Compare true and imputed values\n",
    "print(\"True Value:\", true_value)\n",
    "print(\"Imputed Value:\", features_mean_imputed[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "用替代值替换缺失数据有两种主要策略，每种策略各有优缺点。首先，我们可以使用机器学习来预测缺失数据的值。为此，我们将缺失值的特征作为目标向量，并使用剩余的特征子集来预测缺失值。虽然我们可以使用广泛的机器学习算法来插补值，但一个流行的选择是KNN。KNN在后面进行了深入的讨论，但简短的解释是，该算法使用k个最近观测值（根据某种距离度量）来预测缺失值。在我们的解决方案中，我们使用五个最近的观测值来预测漏损值\n",
    "KNN的缺点是为了知道哪些观测值最接近缺失值，它需要计算缺失值与每个观测值之间的距离。这在较小的数据集中是合理的，但如果一个数据集有数百万个观测值，则很快就会出现问题。\n",
    "另一种更具伸缩性的策略是用一些平均值填充所有缺失的值。例如，在我们的解决方案中，我们使用scikit learn用特征的平均值填充缺失值。估计值通常不像我们使用KNN时那样接近真实值，但是我们可以很容易地将均值填充到包含数百万个观测值的数据中。\n",
    "如果我们使用插补，最好创建一个二元特征来指示观测值是否包含插补值\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Also\n",
    "* A Study of K-Nearest Neighbor as an Imputation Method (http://conteudo.icmc.usp.br/pessoas/gbatista/files/his2002.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
